{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30539360",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "<img width=\"10%\" alt=\"Naas\" src=\"https://landen.imgix.net/jtci2pxwjczr/assets/5ice39g4.png?w=160\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba2d382",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "# AWS - Send dataframe to S3\n",
    "<a href=\"https://app.naas.ai/user-redirect/naas/downloader?url=https://raw.githubusercontent.com/jupyter-naas/awesome-notebooks/master/AWS/AWS_Send_dataframe_to_S3.ipynb\" target=\"_parent\"><img src=\"https://naasai-public.s3.eu-west-3.amazonaws.com/Open_in_Naas_Lab.svg\"/></a><br><br><a href=\"https://github.com/jupyter-naas/awesome-notebooks/issues/new?assignees=&labels=&template=template-request.md&title=Tool+-+Action+of+the+notebook+\">Template request</a> | <a href=\"https://github.com/jupyter-naas/awesome-notebooks/issues/new?assignees=&labels=bug&template=bug_report.md&title=AWS+-+Send+dataframe+to+S3:+Error+short+description\">Bug report</a> | <a href=\"https://app.naas.ai/user-redirect/naas/downloader?url=https://raw.githubusercontent.com/jupyter-naas/awesome-notebooks/master/Naas/Naas_Start_data_product.ipynb\" target=\"_parent\">Generate Data Product</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a5aae6",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "**Tags:** #aws #cloud #storage #S3bucket #operations #snippet #dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7e082b",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "**Author:** [Maxime Jublou](https://www.linkedin.com/in/maximejublou/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naas-description",
   "metadata": {
    "papermill": {},
    "tags": [
     "description"
    ]
   },
   "source": [
    "**Description:** This notebook demonstrates how to use AWS to send a dataframe to an S3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb46a1ae",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa3bb90",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745de8d2",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import awswrangler as wr\n",
    "except:\n",
    "    !pip install awswrangler --user\n",
    "    import awswrangler as wr\n",
    "import pandas as pd\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ce42c4",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Setup AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c160ef0",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Credentials\n",
    "AWS_ACCESS_KEY_ID = \"YOUR_AWS_ACCESS_KEY_ID\"\n",
    "AWS_SECRET_ACCESS_KEY = \"YOUR_AWS_SECRET_ACCESS_KEY\"\n",
    "AWS_DEFAULT_REGION = \"YOUR_AWS_DEFAULT_REGION\"\n",
    "\n",
    "# Bucket\n",
    "BUCKET_PATH = f\"s3://naas-data-lake/dataset/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d654e0c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T06:42:00.638182Z",
     "iopub.status.busy": "2022-04-21T06:42:00.637939Z",
     "iopub.status.idle": "2022-04-21T06:42:00.843677Z",
     "shell.execute_reply": "2022-04-21T06:42:00.842680Z",
     "shell.execute_reply.started": "2022-04-21T06:42:00.638157Z"
    },
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Setup Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e7d0f5",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "%env AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID\n",
    "%env AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY\n",
    "%env AWS_DEFAULT_REGION=$AWS_DEFAULT_REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612e695e",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d77884",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Get dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac4e6d3",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": [1, 2],\n",
    "        \"value\": [\"foo\", \"boo\"],\n",
    "        \"date\": [date(2020, 1, 1), date(2020, 1, 2)],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Display dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fef003",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69333139",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Send dataset to S3\n",
    "Wrangler has 3 different write modes to store Parquet Datasets on Amazon S3.\n",
    "- **append** (Default) : Only adds new files without any delete.\n",
    "- **overwrite** : Deletes everything in the target directory and then add new files.\n",
    "- **overwrite_partitions** (Partition Upsert) : Only deletes the paths of partitions that should be updated and then writes the new partitions files. It's like a \"partition Upsert\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601e9f52",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "wr.s3.to_parquet(df=df, path=BUCKET_PATH, dataset=True, mode=\"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "naas": {
   "notebook_id": "0d5fe4330975af3c53c528f914bd8d0124c25dea14afe09df1119b430a9bb2b2",
   "notebook_path": "AWS/AWS_Send_dataframe_to_S3.ipynb"
  },
  "papermill": {
   "default_parameters": {},
   "environment_variables": {},
   "parameters": {},
   "version": "2.3.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}