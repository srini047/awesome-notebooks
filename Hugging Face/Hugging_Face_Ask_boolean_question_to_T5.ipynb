{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "heated-model",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T06:02:01.679489Z",
     "iopub.status.busy": "2021-05-08T06:02:01.679228Z",
     "iopub.status.idle": "2021-05-08T06:02:01.695397Z",
     "shell.execute_reply": "2021-05-08T06:02:01.694380Z",
     "shell.execute_reply.started": "2021-05-08T06:02:01.679428Z"
    },
    "papermill": {},
    "tags": []
   },
   "source": [
    "<img width=\"10%\" alt=\"Naas\" src=\"https://landen.imgix.net/jtci2pxwjczr/assets/5ice39g4.png?w=160\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-professor",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "# Hugging Face - Ask boolean question to T5\n",
    "<a href=\"https://app.naas.ai/user-redirect/naas/downloader?url=https://raw.githubusercontent.com/jupyter-naas/awesome-notebooks/master/Hugging%20Face/Hugging_Face_Ask_boolean_question_to_T5.ipynb\" target=\"_parent\"><img src=\"https://naasai-public.s3.eu-west-3.amazonaws.com/open_in_naas.svg\"/></a><br><br><a href=\"https://github.com/jupyter-naas/awesome-notebooks/issues/new?assignees=&labels=&template=template-request.md&title=Tool+-+Action+of+the+notebook+\">Template request</a> | <a href=\"https://github.com/jupyter-naas/awesome-notebooks/issues/new?assignees=&labels=bug&template=bug_report.md&title=Hugging+Face+-+Ask+boolean+question+to+T5:+Error+short+description\">Bug report</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f6fa04-7426-4971-aad3-59c1e903bfe4",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "**Tags:** #huggingface #ml #sales #ai #text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150bff0b-3d5e-45fe-a010-0058c1a07210",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "**Author:** [Jeremy Ravenel](https://www.linkedin.com/in/ACoAAAJHE7sB5OxuKHuzguZ9L6lfDHqw--cdnJg/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-plastic",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "## T5-base finetuned on BoolQ (superglue task)\n",
    "This notebook is for demonstrating the training and use of the text-to-text-transfer-transformer (better known as T5) on boolean questions (BoolQ). The example use case is a validator indicating if an idea is environmentally friendly. Nearly any question can be passed into the `query` function (see below) as long as a context to a question is given.\n",
    "\n",
    "Author: Maximilian Frank ([script4all.com](//script4all.com)) - Copyleft license\n",
    "\n",
    "Notes:\n",
    "- The model from [huggingface.co/mrm8488/t5-base-finetuned-boolq](//huggingface.co/mrm8488/t5-base-finetuned-boolq) is used in this example as it is an already trained t5-base model on boolean questions (BoolQ task of superglue).\n",
    "- Documentation references on [huggingface.co/transformers/model_doc/t5.html#training](//huggingface.co/transformers/model_doc/t5.html#training), template script on [programming-review.com/machine-learning/t5](//programming-review.com/machine-learning/t5)\n",
    "- The greater the model, the higher the accuracy on BoolQ (see [arxiv.org/pdf/1910.10683.pdf](//arxiv.org/pdf/1910.10683.pdf)):\n",
    "    t5-small|t5-base|t5-large|t5-3B|t5-11B\n",
    "    -|-|-|-|-\n",
    "    76.4%|81.4%|85.4%|89.9%|91.2%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-chart",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "## Loading the model\n",
    "If here comes an error, install the packages via `python3 -m pip install … --user`.\n",
    "\n",
    "You can also load a T5 plain model (not finetuned). Just replace the following code\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "tokenizer = AutoTokenizer.from_pretrained('mrm8488/t5-base-finetuned-boolq')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('mrm8488/t5-base-finetuned-boolq')…\n",
    "```\n",
    "with\n",
    "```python\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "```\n",
    "where `t5-small` is one of the names in the table above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "input_cell",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9704803-b904-4fb8-81f2-c97d35c20dff",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-proceeding",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import_cell",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "equivalent-double",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T05:17:25.351132Z",
     "iopub.status.busy": "2021-05-08T05:17:25.350861Z",
     "iopub.status.idle": "2021-05-08T05:17:27.173947Z",
     "shell.execute_reply": "2021-05-08T05:17:27.173309Z",
     "shell.execute_reply.started": "2021-05-08T05:17:25.351067Z"
    },
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from operator import itemgetter\n",
    "from distutils.util import strtobool\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b55a32-7454-42ce-89ed-7e287a130cce",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "combined-south",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T05:17:29.455469Z",
     "iopub.status.busy": "2021-05-08T05:17:29.455239Z",
     "iopub.status.idle": "2021-05-08T05:17:47.738897Z",
     "shell.execute_reply": "2021-05-08T05:17:47.738226Z",
     "shell.execute_reply.started": "2021-05-08T05:17:29.455444Z"
    },
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-boolq\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"mrm8488/t5-base-finetuned-boolq\").to(\n",
    "    torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    ")\n",
    "try:\n",
    "    model.parallelize()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_cell",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-appointment",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Training\n",
    "> **Optional:** You can leave the following out, if you don't have custom datasets. By default the number of training epochs equals 0, so nothing is trained.\n",
    "\n",
    "> **Warning:** This option consumes a lot of runtime and thus *naas.ai* credits. Make sure to have enough credits on your account.\n",
    "\n",
    "For each dataset a stream-opener has to be provided which is readable line by line (e.g. file, database). In the array with key `keys` are all dictionary keys which exist in the jsonl-line. So in this example the first training dataset has the keys `question` for the questions (string),`passage` for the contexts (string) and `answer` for the answers (boolean). Adjust these keys to your dataset.\n",
    "\n",
    "At last you have to adjust the number of epochs to be trained (see comment `# epochs`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unable-scotland",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T05:17:47.740244Z",
     "iopub.status.busy": "2021-05-08T05:17:47.740027Z",
     "iopub.status.idle": "2021-05-08T05:17:47.751063Z",
     "shell.execute_reply": "2021-05-08T05:17:47.750542Z",
     "shell.execute_reply.started": "2021-05-08T05:17:47.740215Z"
    },
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "srcs = [\n",
    "    {\n",
    "        \"stream\": lambda: open(\"boolq/train.jsonl\", \"r\"),\n",
    "        \"keys\": [\"question\", \"passage\", \"answer\"],\n",
    "    },\n",
    "    {\n",
    "        \"stream\": lambda: open(\"boolq/dev.jsonl\", \"r\"),\n",
    "        \"keys\": [\"question\", \"passage\", \"answer\"],\n",
    "    },\n",
    "    {\n",
    "        \"stream\": lambda: open(\"boolq-nat-perturb/train.jsonl\", \"r\"),\n",
    "        \"keys\": [\"question\", \"passage\", \"roberta_hard\"],\n",
    "    },\n",
    "]\n",
    "model.train()\n",
    "for _ in range(0):  # epochs\n",
    "    for src in srcs:\n",
    "        with src[\"stream\"]() as s:\n",
    "            for d in s:\n",
    "                q, p, a = itemgetter(src[\"keys\"][0], src[\"keys\"][1], src[\"keys\"][2])(\n",
    "                    json.loads(d)\n",
    "                )\n",
    "                tokens = tokenizer(\n",
    "                    \"question:\" + q + \"\\ncontext:\" + p, return_tensors=\"pt\"\n",
    "                )\n",
    "                if len(tokens.input_ids[0]) > model.config.n_positions:\n",
    "                    continue\n",
    "                model(\n",
    "                    input_ids=tokens.input_ids,\n",
    "                    labels=tokenizer(str(a), return_tensors=\"pt\").input_ids,\n",
    "                    attention_mask=tokens.attention_mask,\n",
    "                    use_cache=True,\n",
    "                ).loss.backward()\n",
    "model.eval()\n",
    "# ; suppresses long output on jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-analysis",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Define query function\n",
    "As the model is ready, define the querying function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "forced-calvin",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T05:17:47.752470Z",
     "iopub.status.busy": "2021-05-08T05:17:47.752250Z",
     "iopub.status.idle": "2021-05-08T05:17:47.845461Z",
     "shell.execute_reply": "2021-05-08T05:17:47.844870Z",
     "shell.execute_reply.started": "2021-05-08T05:17:47.752441Z"
    },
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query(q=\"question\", c=\"context\"):\n",
    "    return strtobool(\n",
    "        tokenizer.decode(\n",
    "            token_ids=model.generate(\n",
    "                input_ids=tokenizer.encode(\n",
    "                    \"question:\" + q + \"\\ncontext:\" + c, return_tensors=\"pt\"\n",
    "                )\n",
    "            )[0],\n",
    "            skip_special_tokens=True,\n",
    "            max_length=3,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "output_cell",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-legislature",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Querying on the task\n",
    "Now the actual task begins: Query the model with your ideas (see list `ideas`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "invalid-flood",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T05:39:54.534227Z",
     "iopub.status.busy": "2021-05-08T05:39:54.533994Z",
     "iopub.status.idle": "2021-05-08T05:39:59.255031Z",
     "shell.execute_reply": "2021-05-08T05:39:59.254404Z",
     "shell.execute_reply.started": "2021-05-08T05:39:54.534199Z"
    },
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    ideas = [\n",
    "        \"The idea is to pollute the air instead of riding the bike.\",  # should be false\n",
    "        \"The idea is to go cycling instead of driving the car.\",  # should be true\n",
    "        \"The idea is to put your trash everywhere.\",  # should be false\n",
    "        \"The idea is to reduce transport distances.\",  # should be true\n",
    "        \"The idea is to put plants on all the roofs.\",  # should be true\n",
    "        \"The idea is to forbid opensource vaccines.\",  # should be true\n",
    "        \"The idea is to go buy an Iphone every five years.\",  # should be false\n",
    "        \"The idea is to walk once every week in the nature.\",  # should be true\n",
    "        \"The idea is to go buy Green bonds.\",  # should be true\n",
    "        \"The idea is to go buy fast fashion.\",  # should be false\n",
    "        \"The idea is to buy single-use items.\",  # should be false\n",
    "        \"The idea is to drink plastic bottled water.\",  # should be false\n",
    "        \"The idea is to use import goods.\",  # should be false\n",
    "        \"The idea is to use buy more food than you need.\",  # should be false\n",
    "        \"The idea is to eat a lot of meat.\",  # should be false\n",
    "        \"The idea is to eat less meat.\",  # should be false\n",
    "        \"The idea is to always travel by plane.\",  # should be false\n",
    "        \"The idea is to opensource vaccines.\",  # should be false\n",
    "    ]\n",
    "    for idea in ideas:\n",
    "        print(\"🌏 Idea:\", idea)\n",
    "        print(\n",
    "            \"\\t✅ Good idea\"\n",
    "            if query(\"Is the idea environmentally friendly?\", idea)\n",
    "            else \"\\t❌ Bad idea\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "naas": {
   "notebook_id": "502986fc-65ec-4e80-8765-c406cf4a483f",
   "notebook_path": "Hugging Face/Hugging_Face_Ask_boolean_question_to_T5.ipynb"
  },
  "papermill": {
   "default_parameters": {},
   "environment_variables": {},
   "parameters": {},
   "version": "2.3.3"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}